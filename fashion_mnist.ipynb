{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(data.Dataset):\n",
    "    def __init__(self, path, kind='train'):\n",
    "        labels_path = os.path.join(path, kind + '-labels-idx1-ubyte')\n",
    "        self.labels = np.frombuffer(open(labels_path, 'rb').read(), dtype=np.uint8, offset=8)\n",
    "        images_path = os.path.join(path, kind + '-images-idx3-ubyte')\n",
    "        self.images = np.frombuffer(open(images_path, 'rb').read(), dtype=np.uint8, offset=16).reshape(len(self.labels), 28, 28)\n",
    "        assert len(self.images) == len(self.labels)\n",
    "        self.class_names = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'boot']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        img = imresize(img, [32, 32])\n",
    "        img = np.reshape(img, [1, 32, 32])\n",
    "        return img, self.labels[index]\n",
    "\n",
    "path = 'data/fashion_mnist'\n",
    "dataset = FashionMNIST(path)\n",
    "test_dataset = FashionMNIST(path, 't10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32)\n",
      "(32, 32, 3)\n",
      "pullover\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1103592b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show(img):\n",
    "    # input: CxWxH\n",
    "    print(img.shape)\n",
    "    whc = np.transpose(img, [1,2,0])\n",
    "    rgb = np.tile(whc, [1,1,3])\n",
    "    print(rgb.shape)\n",
    "    imshow(rgb)\n",
    "\n",
    "img1, label1 = dataset[5]\n",
    "show(img1)\n",
    "print(dataset.class_names[label1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # starts off 28x28x1\n",
    "        \n",
    "        def layer(in_chans, out_chans):\n",
    "            return [\n",
    "                nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(out_chans),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ]\n",
    "        \n",
    "        # 32x32x1 -> 16x16x32 -> 8x8x64 -> 4x4x128 -> 2x2x128\n",
    "        ops = layer(1, 32) + layer(32, 64) + layer(64, 128) + layer(128, 128)\n",
    "        self.features = nn.Sequential(*ops)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2*2*128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 2*2*128)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: Variable containing:\n",
      " 2.4573\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "20 loss: Variable containing:\n",
      " 1.4689\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "40 loss: Variable containing:\n",
      " 1.1071\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "60 loss: Variable containing:\n",
      " 0.8149\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "80 loss: Variable containing:\n",
      " 0.5013\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "100 loss: Variable containing:\n",
      " 1.0131\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "120 loss: Variable containing:\n",
      " 0.4976\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "140 loss: Variable containing:\n",
      " 0.6990\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "160 loss: Variable containing:\n",
      " 0.5137\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "180 loss: Variable containing:\n",
      " 0.4686\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "200 loss: Variable containing:\n",
      " 0.5670\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "220 loss: Variable containing:\n",
      " 0.6001\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "240 loss: Variable containing:\n",
      " 0.5183\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "260 loss: Variable containing:\n",
      " 1.3112\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "280 loss: Variable containing:\n",
      " 1.2424\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "300 loss: Variable containing:\n",
      " 1.0047\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "320 loss: Variable containing:\n",
      " 0.3444\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "340 loss: Variable containing:\n",
      " 0.3247\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "360 loss: Variable containing:\n",
      " 0.8448\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "380 loss: Variable containing:\n",
      " 0.2604\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "400 loss: Variable containing:\n",
      " 0.1840\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "420 loss: Variable containing:\n",
      " 0.5501\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "440 loss: Variable containing:\n",
      " 0.9691\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "460 loss: Variable containing:\n",
      " 0.8507\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "480 loss: Variable containing:\n",
      " 0.5227\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "500 loss: Variable containing:\n",
      " 0.3933\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "520 loss: Variable containing:\n",
      " 0.4304\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "540 loss: Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.1620\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "560 loss: Variable containing:\n",
      " 0.7036\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "580 loss: Variable containing:\n",
      " 0.3902\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "600 loss: Variable containing:\n",
      " 0.5768\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "620 loss: Variable containing:\n",
      " 0.2910\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "640 loss: Variable containing:\n",
      " 0.5029\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "660 loss: Variable containing:\n",
      " 0.3688\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "680 loss: Variable containing:\n",
      " 0.6943\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "700 loss: Variable containing:\n",
      " 0.4482\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "720 loss: Variable containing:\n",
      " 0.5542\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "740 loss: Variable containing:\n",
      " 0.3527\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "760 loss: Variable containing:\n",
      " 0.5916\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "780 loss: Variable containing:\n",
      " 0.9531\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-43ee09536807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-43ee09536807>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_batches, print_n)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loader = data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(n_batches=1, print_n=20):\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(loader):\n",
    "        if idx > n_batches: break\n",
    "        data = Variable(data.type(torch.FloatTensor))\n",
    "        target = Variable(target.type(torch.LongTensor))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % print_n == 0 or idx == n_batches-1:\n",
    "            print(idx, \"loss:\", loss)\n",
    "\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8480392156862745\n"
     ]
    }
   ],
   "source": [
    "test_loader = data.DataLoader(dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "def accuracy():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        data = Variable(data.type(torch.FloatTensor))\n",
    "        predictions = model(data).data.max(1)[1]\n",
    "        correct += predictions.eq(target.type(torch.LongTensor)).sum()\n",
    "        total += len(target)\n",
    "        if idx > 100: break\n",
    "    return correct / total\n",
    "\n",
    "print(\"Accuracy:\", accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
